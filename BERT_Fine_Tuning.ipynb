{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X2HqEHzyS5o"
      },
      "source": [
        "# POS TECH - IA PARA DEVS\n",
        "### Tech Challenge - Fase 03\n",
        "\n",
        "**Aluno:** Inacio Ribeiro - RM362328\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COiPOpFDyUeY"
      },
      "source": [
        "O objetivo deste Tech Challenge é executar o processo de fine-tuning de um modelo de fundação (BERT) utilizando o dataset \"The Amazon Titles-1.3MM\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z4uDodz7zAML",
        "scrolled": true,
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760122115178,
          "user_tz": 180,
          "elapsed": 6896,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets evaluate scikit-learn -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": [],
        "id": "olINM90RAgPR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760136029254,
          "user_tz": 180,
          "elapsed": 164,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline\n",
        ")\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-lfphOozfOo"
      },
      "source": [
        "### 1. Importando o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "ec18b5a520dc4760b7097d1b154cb953",
            "21a87548bbf5426c95d53179a6e34a8f",
            "98b11d1c9c2d4a3cb72a7c0a09eb3aec",
            "75d77d36e5c64ccc86b719da12f6c2c2",
            "2aad3afe2f3541309ada287366179456",
            "53ccae4dd5434ca99d70314dee25a2eb",
            "3e724a5dd9784644b700c30253bed34b",
            "d65fe3ec76524dd4a6f4394d6c4c7f59",
            "4826fbc7e5984bde88148d13dac5c7cd",
            "fcf5b5127d5d47beb2e07ea0ad930c9c",
            "a25fc6d7a4484c26894ea444973fb9f5"
          ]
        },
        "id": "fT8ss-2Dziza",
        "outputId": "0f793e4a-9e2f-4b84-8b75-e3d0590159b4",
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760122148238,
          "user_tz": 180,
          "elapsed": 235,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec18b5a520dc4760b7097d1b154cb953"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer'],\n",
            "    num_rows: 7529\n",
            "})\n",
            "{'question': \"What is the product 'Girls Ballet Tutu Neon Pink'?\", 'answer': 'High quality 3 layer ballet tutu. 12 inches in length'}\n"
          ]
        }
      ],
      "source": [
        "file_path = '/home/trn.json'\n",
        "\n",
        "MAX_RECORDS = 10000\n",
        "\n",
        "data = []\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= MAX_RECORDS:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            data.append(json.loads(line))\n",
        "        except json.JSONDecodeError as e:\n",
        "            continue\n",
        "\n",
        "records = []\n",
        "for entry in data:\n",
        "    title = entry.get(\"title\", \"\").strip()\n",
        "    content = entry.get(\"content\", \"\").strip()\n",
        "\n",
        "    if not content:\n",
        "        continue\n",
        "\n",
        "    question = f\"What is the product '{title}'?\"\n",
        "    answer = content\n",
        "    records.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "dataset = Dataset.from_list(records)\n",
        "dataset.to_json(\"/home/data.json\")\n",
        "print(dataset)\n",
        "print(dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_path = \"/home/data.json\"\n",
        "output_path = \"/home/data_list.json\"\n",
        "\n",
        "with open(input_path, \"r\") as infile:\n",
        "    raw = infile.read()\n",
        "\n",
        "entries = []\n",
        "for item in raw.strip().split(\"}\\n{\"):\n",
        "    item = item.strip()\n",
        "    if not item.startswith(\"{\"):\n",
        "        item = \"{\" + item\n",
        "    if not item.endswith(\"}\"):\n",
        "        item = item + \"}\"\n",
        "    try:\n",
        "        entries.append(json.loads(item))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Skipping invalid entry: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(entries, outfile, indent=2)\n",
        "\n",
        "\n",
        "print(f\"Converted {len(entries)} entries to JSON list at: {output_path}\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/home/data_list.json\", split=\"train\")\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "831e9976385d4780aedea2a476dc6884",
            "c415cac771e44bc99d7f7cfca720ad7b",
            "fb88f080084241dcba9832d4187c77bc",
            "51c37d9ce2be479ba401895ecf4196ce",
            "ec50daf1c561417aaf4e5eaa68d42dc7",
            "8a864ed2d1914bec9f6ab0bfd41889e6",
            "6d423f4f98434d97a6482abf31d63a09",
            "cda99ee4b68548079a28a49607d2d7c7",
            "f070f3061ea8464b892a19ff98a4bdc8",
            "81e15a8630674a6d8b3a901475011f25",
            "a26d08d9dc77418d8d16d4a6bf4dad4a"
          ]
        },
        "id": "_rxNk8ajfBYr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760123861385,
          "user_tz": 180,
          "elapsed": 612,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ee584845-0d57-4c91-bfbc-4161f9443be4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 7529 entries to JSON list at: /home/data_list.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "831e9976385d4780aedea2a476dc6884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': \"What is the product 'Girls Ballet Tutu Neon Pink'?\", 'answer': 'High quality 3 layer ballet tutu. 12 inches in length'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/home/data_list.json\"\n",
        "output_path = \"/home/data_list.json\"\n",
        "\n",
        "with open(input_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for entry in data:\n",
        "    entry[\"context\"] = entry[\"answer\"]\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=2)"
      ],
      "metadata": {
        "id": "txHnV_UujuTr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760123900886,
          "user_tz": 180,
          "elapsed": 178,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJq7pE2949K4"
      },
      "source": [
        "### 2. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "aa2d2e8c609144c3bf6a61f0875bef67",
            "e7bcda98ddb14de9a6566941153baee3",
            "a47bbcf86e584729815522599ba9c378",
            "534eff97e9844465b867f1b9b8ec5b3b",
            "7c4d32cd4f4b422aaf872ff75f373c2d",
            "d9cabef40fa342acb2f8a64e47f43f65",
            "cecce8f3062441c8b438eca45202b21d",
            "27f21f03f15e40ad92eda22cac61f483",
            "883695eaf6314b07b0567dcde10901b0",
            "dad6ce9fc389427a9475882d5dd0d37c",
            "0aeb057f2d95498db2b61f7fefd9afce",
            "988aeaa8af31431cbfa3857164ee6d03",
            "662586956ec24b46bc8caec32ff0e00e",
            "d74f263c9f2b4a9c9666d1f3cbacfd74",
            "890cb4bbd5d14b22ac329f4d3082bdca",
            "e079a9bcf7734299a418ac10833490aa",
            "021a5bfe5faa4429836e7d2f9c6fef08",
            "56b3e011c01544adbdd06b3e995ae606",
            "ec0c6623703340d9b6ac6221ae48add1",
            "136c5f42112b4b85a3a406020fc1219e",
            "06f49fafc46444949f89e851029214a9",
            "c61b501825ae4e5c8efab61c613d0bd8"
          ]
        },
        "id": "gPRrHcAa5Gc0",
        "outputId": "fe135911-068b-4d60-baef-58a6a2e43d57",
        "scrolled": true,
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760123930167,
          "user_tz": 180,
          "elapsed": 10506,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa2d2e8c609144c3bf6a61f0875bef67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7529 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "988aeaa8af31431cbfa3857164ee6d03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
            "    num_rows: 7529\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/home/data_list.json\", split=\"train\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def prepare_train_features(example):\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "    answer_text = example[\"answer\"]\n",
        "\n",
        "    start_char = context.find(answer_text)\n",
        "    if start_char == -1:\n",
        "        start_char = 0\n",
        "        end_char = 0\n",
        "    else:\n",
        "        end_char = start_char + len(answer_text)\n",
        "\n",
        "    tokenized_example = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        truncation=True,\n",
        "        max_length=384,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offsets = tokenized_example[\"offset_mapping\"]\n",
        "\n",
        "    start_token = end_token = 0\n",
        "    for idx, (start, end) in enumerate(offsets):\n",
        "        if start <= start_char < end:\n",
        "            start_token = idx\n",
        "        if start < end_char <= end:\n",
        "            end_token = idx\n",
        "\n",
        "    tokenized_example[\"start_positions\"] = start_token\n",
        "    tokenized_example[\"end_positions\"] = end_token\n",
        "    tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "tokenized_dataset = dataset.map(prepare_train_features)\n",
        "\n",
        "print(tokenized_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTyubMviAgPT"
      },
      "source": [
        "### 3. Argumentos de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": [],
        "id": "5Knz76TmAgPT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760126074635,
          "user_tz": 180,
          "elapsed": 1494,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53ef1af-938c-47f0-efc2-19a67d960150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForQuestionAnswering, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    start_preds = np.argmax(p.predictions[0], axis=1)\n",
        "    end_preds = np.argmax(p.predictions[1], axis=1)\n",
        "\n",
        "    start_labels, end_labels = p.label_ids\n",
        "\n",
        "    return {\n",
        "        \"start_accuracy\": (start_preds == start_labels).mean(),\n",
        "        \"end_accuracy\": (end_preds == end_labels).mean()\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHkv4xx9AgPT"
      },
      "source": [
        "### 4. Treinamento Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [],
        "id": "U9eQy6cQAgPT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760126811370,
          "user_tz": 180,
          "elapsed": 615091,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "7ea671e1-fcd4-4709-8256-e8b3ca389161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1413' max='1413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1413/1413 10:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Start Accuracy</th>\n",
              "      <th>End Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.005400</td>\n",
              "      <td>0.004931</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.009400</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./bert_finetuned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHtCTr1UAgPT"
      },
      "source": [
        "### 5. Testando antes e depois do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NAYCUbgUAgPU",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1760131320472,
          "user_tz": 180,
          "elapsed": 1398,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3148fbea-a525-4e12-8457-e67874e05602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained model answer:  \n",
            "Fine-tuned model answer:  the girls ballet tutu neon pink is a high - quality 3 - layer ballet tutu. 12 inches in length, soft material for comfort.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model_base = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model_finetuned = BertForQuestionAnswering.from_pretrained(\"./bert_finetuned\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_base.to(device)\n",
        "model_finetuned.to(device)\n",
        "\n",
        "question = \"What features the 'Girls Ballet Tutu Neon Pink' have?\"\n",
        "context = \"The Girls Ballet Tutu Neon Pink is a high-quality 3-layer ballet tutu. 12 inches in length, soft material for comfort.\"\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_base = model_base(**inputs)\n",
        "    outputs_finetuned = model_finetuned(**inputs)\n",
        "\n",
        "start_base = torch.argmax(outputs_base.start_logits, dim=1)\n",
        "end_base = torch.argmax(outputs_base.end_logits, dim=1)\n",
        "\n",
        "start_finetuned = torch.argmax(outputs_finetuned.start_logits, dim=1)\n",
        "end_finetuned = torch.argmax(outputs_finetuned.end_logits, dim=1)\n",
        "\n",
        "answer_base = tokenizer.decode(inputs[\"input_ids\"][0][start_base:end_base+1])\n",
        "answer_finetuned = tokenizer.decode(inputs[\"input_ids\"][0][start_finetuned:end_finetuned+1])\n",
        "\n",
        "print(\"Pretrained model answer: \", answer_base)\n",
        "print(\"Fine-tuned model answer: \", answer_finetuned)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "BERT_Fine_Tuning.ipynb"
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m134",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }

  },
  "nbformat": 4,
  "nbformat_minor": 0
}